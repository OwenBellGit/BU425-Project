---
title: "BU425_project_v1"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# import libraries
library(ggplot2)
library(dplyr)
library(GGally)
library(ggcorrplot)
library(ROSE)
library(caret)
library(randomForest)
library(scales)
library(rpart)
#install.packages("glmnet")
library(glmnet)
library(rpart)
library(pROC)
library(ROSE)
```

## R Markdown

# Telecom Churn Data

## 1. Introduction
Business Problem: 
Customers from a Telecommunication company are no longer using the company’s services. This proves to be a business problem since the company is losing revenue for every customer that leaves. Aside from lost business, customer churn has a negative impact as it gives the company a bad reputation. This could deter future customers from wanting to use the company’s services/buy the products as they know that their past customers were not satisfied and chose to discontinue the use of services/purchase products. Customers leaving could also be the result of an underlying problem, such as having a bad product or being overpriced. That reason it’s worth looking into this business problem is that there are multiple factors at play that could be hurting a company with a high churn rate.  

Analytic Problem:
The analytical problem in the case of analyzing churn rate would be determining which factors are common between customers that are choosing to discontinue the use of products or services offered by the company. The factors in this analysis can be used to predict whether a customer is likely to leave, which will allow the company the opportunity to take mitigating action.  


Goal: 
The main goal here is to deal with the issue of churn rate as it can have a negative impact on the company in several regards. We hope to use machine learning algorithms to predict if certain customers will churn and the ways in which it can be prevented. 


## 2. Data Description

We used sample data from IBM for our customer churn case. The dataset contains over 7,000 records and 33 columns and represents customer data from a telecommunication company based in California, USA.

```{r}
#import data
initial_data<- read.csv("Telco_customer_churn.csv")
head(initial_data)

# Look at the makeup of the data
str(initial_data)
# 7043 rows and 33 columns 
# target variable: Churn Label / Churn Value
```

### Dealing with Missing Values

One of the first things we need to do is determine the type and quality of the data. We want to ensure that any data fields used in model-building are also available on a consistent basis when the model is deployed. We should also ensure that the data does not include any personal data that could be used to identify individuals. We noticed that there are several columns which do not provide new or useful information for instance, all customers in this data reside in California, so columns for State and Country provide no value. Next, we would look for any missing values, and determine whether these records should be dropped, or if values can be imputed from existing information.  

```{r}
# check for missing values
colSums(is.na(initial_data))
#can see that total charges has 11 observations missing
```

```{r}
# shows the rows in which total charges is NA
missing_val<- initial_data[is.na(initial_data$Total.Charges),]
missing_val
# these missing values are when churn = 0 so they are staying with the company
```

```{r}
# dropping the NA values, would not contribute much in our findings as we have about 73% of the data that is not churn (churn = 0)
data<- na.omit(initial_data)
colSums(is.na(data))
```

```{r}
# removing columns that are not providing useful information
data<- data%>% select(-State, -Country)
```

```{r}
# count the target variable
initial_data %>%
  group_by(Churn.Label) %>%
  summarise(count = n(), 
            percentage = paste0(round(count/nrow(initial_data)*100,0),"%"),
            .groups="drop")
```

The target variable found in the “Churn Value” column indicates whether or not a customer stays or leaves the company.  On reviewing the data, we noticed that the dataset is imbalanced with about 73% staying with the company compared to 27% who leave. We will have to take this into account when doing our analysis.  


## 3. Exploratory Data Analysis
There are several different data fields that are available and we have done some preliminary exploratory data analysis, to see how they relate to the target, and whether there are any variable correlations. For all customers we have information on characteristics like gender, whether they have a partner or any dependents, and how long they are with the company. We also have information about the different types of services that the company provides such as phone, internet, streaming or a combination of these.   

```{r}
# distribution of target variable 
data %>% 
  ggplot(aes(x = Churn.Label, fill = Churn.Label))+
  geom_bar(aes(y = after_stat(count)/sum(after_stat(count))))+
  ylab("Proportion")+
  xlab("Churn Value")+
  ggtitle("Distribution of Churn Rate")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")
```

The graph below show there are similar numbers of customers by gender, with each having similar proportions that leave versus remaining with the company.
```{r}
data %>% 
  group_by(Gender, Churn.Label) %>% 
  summarise(count = n(),
            .groups = "drop") %>% 
  mutate(prop = paste0(round(count/sum(count)*100, 0), '%')) %>% 
  ggplot(aes(x = Gender, y = count, fill = Churn.Label)) +
  geom_col(position = "stack") +
  geom_text(aes(label = prop), vjust = 1, size = 3) +
  ggtitle("Counts of Churn Rate by Gender")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")
```
The graph below show the majority of customers are not senior citizens, but those that are have a higher likelihood to leave.
```{r}
# distribution by senior for churn rate
data %>% 
  group_by(Senior.Citizen, Churn.Label) %>% 
  summarise(count = n(),
            .groups = "drop") %>% 
  mutate(prop = paste0(round(count/sum(count)*100, 0), '%')) %>% 
  ggplot(aes(x = Senior.Citizen, y = count, fill = Churn.Label)) +
  geom_col(position = "stack") +
  geom_text(aes(label = prop), vjust = 1, size = 3) +
  ggtitle("Counts of Churn Rate by Senior Citizen category")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")
```

This distribution shows that there are more people leaving the company with a contract that is month-to month vs those that are fixed for one or two years. This makes sense because they are paying buy however many months they want the service for.
```{r}
data %>% 
  group_by(Contract, Churn.Label) %>% 
  summarise(count = n(),
            .groups = "drop") %>% 
  mutate(prop = paste0(round(count/sum(count)*100, 0), '%')) %>% 
  ggplot(aes(x = reorder(Contract, -count), y = count, fill = Churn.Label)) +
  geom_col(position = "stack") +
  geom_text(aes(label = prop), vjust = 1, size = 3) +
  ggtitle("Counts of Churn Rate by Type of Contract")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")+
  xlab("Type of Contract")
```

Most customers have Fibre Optic internet services, but this category also has the largest proportion of customers that leave the company
```{r}
data %>% 
  group_by(Internet.Service, Churn.Label) %>% 
  summarise(count = n(),
            .groups = "drop") %>% 
  mutate(prop = paste0(round(count/sum(count)*100, 0), '%')) %>% 
  ggplot(aes(x = reorder(Internet.Service, -count), y = count, fill = Churn.Label)) +
  geom_col(position = "stack") +
  geom_text(aes(label = prop), vjust = 1, size = 3) +
  ggtitle("Counts of Churn Rate by Internet Service") +
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")+
  xlab("Type of Internet Services")
```


Based on these results the majority of customers choose to pay by electronic cheque. However, this also leads to higher likelihood of customer loss.
```{r}
data %>% 
  group_by(Payment.Method, Churn.Label) %>% 
  summarise(count = n(),
            .groups = "drop") %>% 
  mutate(prop = paste0(round(count/sum(count)*100, 0), '%')) %>% 
  ggplot(aes(x = reorder(Payment.Method, count), y = count, fill = Churn.Label)) +
  geom_col(position = "stack") +
  geom_text(aes(label = prop), vjust = 1, size = 3) +
  ggtitle("Counts of Churn Rate by Payment Method")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")+
  xlab("Payment Method")+
  coord_flip()
```

Customers who have been with the company for a shorter period of time are more likely to leave, with a median of 10 months.
```{r}
# plots for continuous variables
data %>%
  ggplot(aes(x = Tenure.Months, fill = Churn.Label))+
  geom_histogram(bins = 30)+
  ggtitle("Distribution of Tenure by Churn Label")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")+
  xlab("Tenure Months")
```
Although there are more outliers, it appears that customers who left the company had a lower median total charge than those who remained with the company. This may indicate that cost alone may not be the motivating factor that causes customers to leave.
```{r}
data %>%
  ggplot(aes(x = Total.Charges, y = Churn.Label, fill = Churn.Label))+
  geom_boxplot()+
  ggtitle("Distribution of Total Charges by Churn Label")+
  theme(plot.title = element_text(hjust = 0.5))+ # centers the title
  scale_fill_brewer(palette = "Set2")

```

For customers who have left, the data also includes a reason for leaving, while this cannot be used in prediction this can be helpful in determining what incentives or mitigating actions could be taken by the company to promote customer retention. About 10% of the customers who left, did so due to price or charges

```{r}
# reason for leaving
data %>% 
  filter(Churn.Value == 1) %>% 
  group_by(Churn.Reason) %>% 
  summarise(count = n(),
           .groups = 'drop') %>% 
  mutate(perc =paste0(round(count / sum(count)*100, 0),'%')) %>% 
  arrange(desc(count))
```


##Feature Engineering
```{r}
#Create a location variable from first 3 digits of zip code
#California can be divided into North and South
#Zip codes < 935** considered South, else considered North

data <- data %>% 
  mutate(Region = if_else(Zip.Code <= 93500, "South", "North"))
       
```


After doing some exploratory data analysis we realized some columns were not providing us additional information. 

```{r}
# need to drop specific columns 
# (do we need both churn value and churn label?), lat long (we have the latitude and longitude separated) , state (all California), Country (all in the United States)
newdata<- data %>% 
  select(c(Gender: Total.Charges, Region, Churn.Label))
head(newdata)

# data that won't be available for why they left (can't use for prediction) 
```

```{r}
# rename the columns
names(newdata) <- c("Gender", "Senior",
                    "Partner", "Dependents",
                    "Tenure","PhoneSvc",
                    "MultLines","InternetSvc",
                    "OLSecurity","OLBackup",
                    "DeviceProt","TechSupport",
                    "StreamTV","StreamMovies",
                    "Contract", "Paperless",
                    "PayMethod", "MonthlyCharges",
                    "TotalCharges","Region", "ChurnLabel")

head(newdata)
```

```{r}
#Convert character fields to categorical variables
newdata <- newdata %>% 
  mutate(MultLines = if_else(PhoneSvc=="No", "No", MultLines),
         OLSecurity = if_else(InternetSvc=="No", "No", OLSecurity),
         OLBackup = if_else(InternetSvc=="No", "No", OLBackup),
         DeviceProt = if_else(InternetSvc=="No", "No", DeviceProt),
         TechSupport = if_else(InternetSvc=="No", "No", TechSupport),
         StreamTV = if_else(InternetSvc=="No", "No", StreamTV),
         StreamMovies = if_else(InternetSvc=="No", "No", StreamMovies))

#Convert character fields to categorical variables
#Set reference level for target variable
newdata <- newdata %>% 
  mutate_if(~is.character(.),as.factor) %>% 
  mutate(ChurnLabel = relevel(ChurnLabel, ref = "No"))
contrasts(newdata$ChurnLabel)
str(newdata)
```
```{r}
#Scale continuous vars
newdata_S <- newdata %>% 
  mutate_if(~is.numeric(.),rescale)
```

```{r, fig.width=12, fig.height=12}
#heatmap
model.matrix(~0+.,newdata_S) %>% 
  cor(use="pairwise.complete.obs") %>% 
  ggcorrplot(show.diag=FALSE, type="lower",lab=TRUE, lab_size=5)
```

TP = Customer is leaving
TN = Customer is staying

## 4. Split the data into train and test
We need to split our data into training and test sets. Using and 80/20 ratio of available data with 80% of the being used for the training data and the remaining 20% being used for the test and validation data. The training data will be what we use to develop and build the model on, and the test data will be used for evaluating the model and doing any hyper tuning (to choose the best parameters) while the validation data will evaluate the final models. 

One consideration we must make is the fact that the data we are working is skewed about 70/30 to customer staying versus leaving as mentioned previously. We will need to modify how we build these datasets to make sure they still accurately reflect the general environment. If we do not do this, we run the risk of either training data being skewed, leading to a highly inaccurate model, or the test set being skewed leading to inaccurate conclusions we can draw from our models

```{r}
#creating train and test splits
# set.seed(0)
# sample <- sample(c(TRUE, FALSE), 1:nrow(oversampled_data), replace=TRUE, prob=c(0.8,0.2))
# train_data <- oversampled_data[sample, ]
# test_data  <- oversampled_data[!sample, ]

set.seed(0)
splitSample <- sample(1:3, size=nrow(newdata_S), prob=c(0.8,0.1,0.1), replace=TRUE)

train_data <- newdata_S[splitSample==1, ]
test_data <- newdata_S[splitSample==2, ]
val_data <- newdata_S[splitSample==3, ]

#check dimensions of train and test
dim(train_data)
dim(test_data)
dim(val_data)


#check proportion
#Check incidence of target in each set
table(train_data$ChurnLabel)
table(test_data$ChurnLabel)
table(val_data$ChurnLabel)

```


```{r}
# Oversample on the train ONLY
oversampleTrain<- ovun.sample(ChurnLabel ~ ., data = train_data, method = "over", seed = 0)$data #oversampled_data

table(oversampleTrain$ChurnLabel)
table(train_data$ChurnLabel)

```


## 5. Build the models
Due to the nature of Churn being one of two options either a customer stays or leaves we decided it fits best to use a classification model  

```{r}
#Functions to be used in model evaluation
getPreds <- function(model, tool, data, cutoff=0.5){
  if (tool =="lr"){
    preds <- predict(model, data, type = "response")
    preds <-ifelse(preds > cutoff,"Yes","No")
    preds <- as.factor(preds)
  }
  else {
    preds <- predict(model, newdata = data)
  }
}

getError <- function(preds, target){
  error = mean(target != preds)
  return(error)
}


getMetrics <- function(preds, data){
  cm <- confusionMatrix(preds, data, positive="Yes")
  print(cm)
  accuracy <- cm$overall[1]
  precision <- cm$byClass[5]
  sensitivity <- cm$byClass[1]
  specificity <- cm$byClass[2]
  print(t(as.matrix(c(accuracy, precision, sensitivity, specificity))))
}
  
```


### Logistic Regression
Logistic Regression was chosen due to its high interpretability and simplicity. Using the predictors, the model will produce a probability between 0 and 1 and from there we can determine if the customer is closer to leaving (1) or staying (0). It is a model that we all have experience with and feel comfortable with using and explaining the results to management and other stakeholders of the telecom company.   The model is easy to improve on through regularization techniques such as Lasso and Ridge. 
```{r}
#build Logistic Regression

#logistic regression model and predictions based on test data
modelLR <- glm(ChurnLabel ~. ,data = oversampleTrain, family = "binomial")
summary(modelLR)
```

```{r}
# to clean this up created functions above 
# pred_train <- predict(m1,oversampleTrain, type = "response")
# pred_test <- predict(m1,test_data, type = "response")

# #probabilities to values
# pred_train<-ifelse(pred_train >0.5,"Yes","No")
# pred_train<-as.factor(pred_train)

# pred_test<-ifelse(pred_test >0.5,"Yes","No")
# pred_test<-as.factor(pred_test)

# tr.err.lr = mean(oversampleTrain$ChurnLabel != pred_train)
# sprintf("Training Error: %f",tr.err.lr)
# ts.err.lr = mean(test_data$ChurnLabel != pred_test)
# sprintf("Testing Error: %f",ts.err.lr)

# Get train and test errors
sprintf("Training Error for Logistic Regression: %f",getError(getPreds(modelLR, "lr", oversampleTrain,0.5), oversampleTrain$ChurnLabel))
sprintf("Testing Error for Logistic Regression: %f",getError(getPreds(modelLR, "lr", test_data,0.5), test_data$ChurnLabel))

```

Training error and testing error are similar, indicating that the model is likely not overfitting. However, there are some features that appear to be not significant as predictors, so we could refine the logistic regression using regularization techniques.

```{r}
# Eval of the model

# too clean up used the method getMetrics created above (help reduce amount of code)
# table(test_data$ChurnLabel, pred_test)
# precision <- table(test_data$ChurnLabel, pred_test)[1]/(table(test_data$ChurnLabel, pred_test)[1] + table(test_data$ChurnLabel, pred_test)[3])
# f1 <- 2 * (precision * sensitivity(test_data$ChurnLabel, pred_test)) / (precision + sensitivity(test_data$ChurnLabel, pred_test))
# 
# sprintf("Accuracy Score: %f",mean(pred_test == test_data$ChurnLabel))
# sprintf("Specificity Score: %f",specificity(test_data$ChurnLabel, pred_test))
# sprintf("Recall/Sensitivity Score: %f",sensitivity(test_data$ChurnLabel, pred_test))
# sprintf("Precision Score: %f",precision)
# sprintf("F1 Score: %f",f1)
# sprintf("Test Error: %f",mean(test_data$ChurnLabel != pred_test))

getMetrics(getPreds(modelLR, "lr", test_data, 0.5), test_data$ChurnLabel)

```
Using Logistic Regression we are able to predict customer churn at an 76% accuracy and a sensitivity of about 80%.

Using cross validataion for Lasso and Ridge Regression
```{r}
# # look at the test errors and hyper tune
# #library(glmnet)
# x<-oversampleTrain%>%select(-ChurnLabel)
# y<- as.numeric(oversampleTrain$ChurnLabel)
# 
# #Lasso
# lasso <- glmnet(as.matrix(x),as.numeric(oversampleTrain$ChurnLabel), alpha = 1, family = "binomial")
# pred_lasso <- predict(lasso,as.matrix(test_data%>%select(-ChurnLabel)), type = "response")
# pred_lasso_train <- predict(lasso,as.matrix(oversampleTrain%>%select(-ChurnLabel)), type = "response")
# pred_lasso<-ifelse(pred_lasso >0.5,"Yes","No")
# pred_lasso_train<-ifelse(pred_lasso_train >0.5,"Yes","No")
# 
# sprintf("Lasso Traing Error: %f",mean(oversampleTrain$ChurnLabel != pred_lasso_train))
# sprintf("Lasso Test Error: %f",mean(test_data$ChurnLabel != pred_lasso))
# sprintf("Lasso Accuracy Score: %f",mean(pred_lasso == test_data$ChurnLabel))
# 
# #Ridge
# ridge <- glmnet(as.matrix(x),as.numeric(oversampleTrain$ChurnLabel), alpha = 0, family = "binomial")
# pred_ridge <- predict(ridge,as.matrix(test_data%>%select(-ChurnLabel)), type = "response")
# pred_ridge_train <- predict(ridge,as.matrix(oversampleTrain%>%select(-ChurnLabel)), type = "response")
# pred_ridge<-ifelse(pred_ridge >0.5,"Yes","No")
# pred_ridge_train<-ifelse(pred_ridge_train>0.5,"Yes","No")
# 
# sprintf("Ridge Traing Error: %f",mean(oversampleTrain$ChurnLabel != pred_ridge_train))
# sprintf("Ridge Test Error: %f",mean(test_data$ChurnLabel != pred_ridge))
# sprintf("Ridge Accuracy Score: %f",mean(pred_ridge == test_data$ChurnLabel))


# the caret package has a ridge and lasso function
# alpha = 0 lasso ; alpha = 1 ridge
tuneGridReg =expand.grid(alpha = c(0,1),
                           lambda=seq(1, 3, by = 0.1))
trainControl <- trainControl(method = "repeatedcv",
                       number = 5,
                       repeats = 3,
                      classProbs = T)

modelReg <- train(ChurnLabel ~ ., data = oversampleTrain, 
            method = "glmnet", 
            trControl = trainControl,
            tuneGrid = tuneGridReg,
            family="binomial")


modelReg
```


There are some features which appear to be insignificant in the model. Removing some of these will help reduce complexity. We will use stepwise AIC to try reducing predictors in the logistic regression model.
```{r}
# added this to help reduce model complexity
library(MASS)
redLRmodel <- stepAIC(modelLR, method="backward")
formula(redLRmodel)
```

```{r}
# using the reduced number of predictors from AIC
finalLR <- glm(ChurnLabel ~ Senior + Partner + Dependents + Tenure + MultLines + InternetSvc + OLSecurity + TechSupport + StreamTV + StreamMovies + Contract + Paperless + PayMethod + MonthlyCharges + TotalCharges, family = "binomial", data = oversampleTrain)

summary(finalLR)
```


This model has a lower AIC than the original logistic regression model, and it is less complex as it uses fewer predictors.

```{r}
#Eval of the model
print(paste0("Training Error for final logistic regression is ", getError(getPreds(finalLR, "lr", oversampleTrain, 0.5), oversampleTrain$ChurnLabel)))
print(paste0("Testing Error for final logistic regression is ", getError(getPreds(finalLR, "lr", test_data, 0.5), test_data$ChurnLabel)))
```


```{r}
getMetrics(getPreds(finalLR, "lr", test_data, 0.5), test_data$ChurnLabel)
```

The logistic regression after using step-wise feature selection produces similar results to the full model. However, it is less complex due to fewer features used and is easier to communicate to management.

### Random Forest
Random Forest is more complicated but produces accurate results especially on imbalanced datasets. The model will tell us if a customer decides to leave or stay based on aggregating predictions from multiple decision trees. The model also does an excellent job of reducing variance and can be scaled easily for future projects.  
```{r}
# build Random Forest
tr_rf<- trainControl(method = "repeatedcv", number = 5, repeats = 3,classProbs=T)
# fitControl <- trainControl(method="none", classProbs=TRUE)
tunegrid_rf <- expand.grid(mtry = seq(3,11,by=2))
```

```{r}
# build Random Forest
# TAKES A WHILE TO RUN
for(i in c(500, 750, 1000)){
  set.seed=0
  modelRF <- train(ChurnLabel~. ,
                   data = oversampleTrain,
                   method = "rf", 
                   trControl = tr_rf,
                   ntrees = i,
                   tuneGrid = tunegrid_rf)
  
  print(modelRF)
}

```

The highest accuracy occurs when mtry=9, so we will fit a random forest model using this value.

```{r}
# Final rf model trained on all train data
set.seed(0)
final_rf <- randomForest(ChurnLabel ~., data = oversampleTrain, mtry = 9, ntree = 750, nodesize = 30)
print(final_rf)
varImpPlot(final_rf)
```

```{r}
# class predictions with test data
# set.seed(0)
# pred_class_rf<-predict(final_rf, newdata = test_data)
# cm_rf<- confusionMatrix(pred_class_rf,factor(test_data$ChurnLabel))
# #cm_rf
# 
# acc_rf<-cm_rf$overall['Accuracy']
# acc_rf
# spec_rf<-cm_rf$byClass['Specificity']
# spec_rf
# sen_rf<-cm_rf$byClass['Sensitivity']
# sen_rf
# prec_rf <- cm_rf$byClass['Precision']
# prec_rf
#Eval of the model

print(paste0("Testing Error for random forest is ", getError(getPreds(final_rf, "rf", test_data, 0.5), test_data$ChurnLabel)))
getMetrics(getPreds(final_rf, "rf", test_data, 0.5), test_data$ChurnLabel)
```

```{r}
# CANT DO TRAIN ERROR USING RF
# # look at the test errors and hyper tune
# pred_rf_train <-predict(final_rf, newdata = oversampleTrain)
# yhat.train.rf = (predict(final_rf,train_data))
# tr.err.rf = mean(oversampleTrain$ChurnLabel != pred_rf_train) 
# tr.err.rf
# 
# pred_rf_test <-predict(final_rf, newdata = test_data)
# yhat.test.rf = (predict(final_rf,test_data))
# ts.err.rf = mean(test_data$ChurnLabel != pred_rf_test) 
# ts.err.rf
# 
# #overfitting?
```


## 6. Evaluate models
Due to of our models being classification models we plan to use the confusion matrix as our main source of evaluation. The confusion matrix consists of the 2x2 table that you can see on the right. The columns being the actual values of what we are trying to predict and the rows being our predictions. The gives us four values: TP (We predict the customer leaves, and they do), FP (We predict they leave, and they do not), TN (We predict they stay, and they do), and finally FN (We predict they stay, but they leave). With these four values we gain a lot of information about our different models, and they offer us many statistics we can use, most importantly the model’s accuracy. Another benefit of the confusion matrix is it offers us the ability to better fine tune our models to better suit the company’s needs. Say it costs less for us to predict that a customer leaves and they do not we would try to tailor our model to maximize for more FP compared to FN. Finally, we can also use an ROC (receiving operating characteristic) curve that provides us with a visual analysis of the model’s results to better compare our different model’s performances.  
```{r}
# Create classification report / confusion matrix to look at values
```

```{r}
# create ROC curve
```


```{r}
# Final test on validation data set
```


## 7. Conclusion

It is important to know how we can use the information gathered in the future to improve. So, for that, let us talk about a Deployment plan. We have come up with 3 steps we think can help. 

Insightful Promotions: We want to utilize the insights from the model to create personalized offers for customers identified as at-risk of leaving the company. One possible way could be segmentation, where we group customers based on churn probability and tailor promotions for each segment. 

Customer Engagement: We will establish mechanisms to gather feedback from customers who were considering leaving but decided to stay due to interventions. Also, conducting regular surveys to gauge customer satisfaction and identify areas for improvement. 

Continuous Monitoring: Implement a system for real-time monitoring of customer behavior and model predictions so that we can continuously update the model to adapt to changing customer preferences and market dynamics. 
